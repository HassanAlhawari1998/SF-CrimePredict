{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, log_loss, mean_squared_error\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Helper.Data import loadData\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Helper.Evaluate_XGBoostModel import evaluate_xgb_model\n",
    "from Helper.Perform_CrossVal import perform_cross_validation\n",
    "from Helper.Perform_GridSearch import perform_grid_search\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T12:58:42.297142400Z",
     "start_time": "2024-02-19T12:58:41.032727100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dataset/train_with_feature.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_train, X_test, y_train, y_test, data, feature_columns, categorical_features,target_column \u001B[38;5;241m=\u001B[39m \u001B[43mloadData\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m X_train\n",
      "File \u001B[1;32m~\\Uni\\Maschine Learning\\SF-CrimePredict\\Helper\\Data.py:12\u001B[0m, in \u001B[0;36mloadData\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloadData\u001B[39m():\n\u001B[0;32m     11\u001B[0m     data_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../Dataset/train_with_feature.csv\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m---> 12\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     data_cleaned \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[0;32m     14\u001B[0m     sample_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20000\u001B[39m\n",
      "File \u001B[1;32mc:\\users\\hassanalhawari\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    900\u001B[0m     dialect,\n\u001B[0;32m    901\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    909\u001B[0m )\n\u001B[0;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\hassanalhawari\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    574\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    576\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 577\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mc:\\users\\hassanalhawari\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\hassanalhawari\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1660\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1661\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1671\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1672\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32mc:\\users\\hassanalhawari\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\common.py:859\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    855\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    856\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    858\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../Dataset/train_with_feature.csv'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, data, feature_columns, categorical_features,target_column = loadData()\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T12:58:43.286658200Z",
     "start_time": "2024-02-19T12:58:42.297142400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the preprocessing stage of our multi-class classification pipeline, we employ a crucial step of transforming categorical target variables into a numerical format. This transformation is essential\n",
    "for compatibility with XGBoost, a gradient boosting framework that requires numerical input to perform mathematical operations and optimizations integral to its learning algorithm (Chen & Guestrin,\n",
    "2016).\n",
    "XGBoost, like many machine learning algorithms, operates on numerical data to execute arithmetic operations, gradient calculations, and optimization procedures. These operations are not inherently\n",
    "defined for categorical data, necessitating a transformation of categorical labels into a numerical representation (Bishop, 2006).\n",
    "Specifically, for a multi-class classification problem, each unique categorical class label is assigned a unique integer value, a process known as label encoding. This procedure transforms the\n",
    "categorical target variable into a format amenable to the mathematical computations required by XGBoost (Hastie, Tibshirani, & Friedman, 2009).\n",
    "For a dataset with \\(C\\) unique classes, label encoding maps each class to a unique integer in the range \\([0, C-1]\\). This approach is both efficient and straightforward, ensuring that the\n",
    "transformed target variable retains the essential categorical information in a numerical format compatible with XGBoost's requirements (James, Witten, Hastie, & Tibshirani, 2013).\n",
    "Upon transformation, XGBoost utilizes the numerical labels to compute a multi-class log loss (cross-entropy loss) when the `multi:softprob` or `multi:softmax` objective function is specified. This\n",
    "loss function quantifies the difference between the predicted probabilities and the actual class labels, guiding the model's learning process (Friedman, 2001).\n",
    "The transformation of categorical data into numerical form is supported by the foundational principles of statistical learning, which emphasize the necessity of numerical representation for the\n",
    "application of mathematical models to data analysis and prediction tasks (Bishop, 2006; Hastie, Tibshirani, & Friedman, 2009). Furthermore, research by Chen and Guestrin (2016) on XGBoost highlights\n",
    "the effectiveness of gradient boosting machines for various tasks, including multi-class classification, when data is appropriately preprocessed and encoded.\n",
    "Data transformation step is not merely a technical requirement but a scientifically grounded practice that enhances the ability of XGBoost to accurately model and predict outcomes in multi-class\n",
    "classification settings. By converting categorical labels into a numerical format, we align our dataset with the underlying mathematical framework of machine learning algorithms, facilitating\n",
    "efficient and effective model training and prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#initialise coder and classifier\n",
    "xgb_clf = XGBClassifier(objective=\"multi:softprob\",use_label_encoder=False, eval_metric='mlogloss')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preliminary testing of machine learning models is crucial during the development phase, providing early insights into model performance and setting the stage for further optimizations.\n",
    "These tests, often conducted via cross-validation this allows us to assess the robustness and generalizability of our models before committing extensive resources to fine-tuning and evaluation.\n",
    "Cross-validation, a common method for preliminary testing, is a powerful tool for assessing model quality.\n",
    "It splits the data into multiple subsets and performs repeated training and testing runs to ensure the model performs reliably on unseen data (Kohavi, 1995).\n",
    "This method helps avoid overfitting and provides an unbiased estimate of model performance.\n",
    "On our research we found out that scientific studies have underscored the importance of early model evaluation in the development process.\n",
    "It allows for the quick identification of inefficient approaches, focusing resources on more promising models (Guyon et al., 2002). We know that one of our project tasks is to implement\n",
    "hyperparameters and improve the model.\n",
    "Therefore, we know that our model may have weaknesses at the beginning and is not the best, but we still want to go one step further and train an early model to gain more insights.\n",
    "Furthermore, research highlights the significance of selecting appropriate metrics for performance evaluation, as they can significantly influence the direction of development and the assessment of model quality (Powers, 2011).\n",
    "We do this because in practice, preliminary tests support the iterative improvement of models by identifying strengths and weaknesses at early stages of development. This approach promotes efficient\n",
    "use of resources and ultimately leads to more powerful and reliable machine learning models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Scoring-Kriterien definieren\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'roc_auc_ovr': 'roc_auc_ovr'\n",
    "}\n",
    "\n",
    "# Kreuzvalidierung mit mehreren Metriken\n",
    "cv_results = cross_validate(xgb_clf, X_train, y_train, cv=4, scoring=scoring, return_train_score=True)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "for metric in scoring.keys():\n",
    "    print(f\"Average {metric}: {np.mean(cv_results[f'test_{metric}']) * 100:.4f}%\")\n",
    "    print(f\"Standard deviation {metric}: {np.std(cv_results[f'test_{metric}']) * 100:.4f}%\")\n",
    "\n",
    "# Prüfen nach Overfitting\n",
    "for metric in scoring.keys():\n",
    "    test_std = np.std(cv_results[f'test_{metric}'])\n",
    "    train_std = np.std(cv_results[f'train_{metric}'])\n",
    "    print(f\"Difference in std between train and test {metric}: {(train_std - test_std) * 100:.4f}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training des Classifiers\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen auf dem Testset\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Vorhersagen der Wahrscheinlichkeiten für das Testset, um ROC-AUC und Log Loss zu berechnen\n",
    "y_pred_proba = xgb_clf.predict_proba(X_test)\n",
    "\n",
    "# Berechnung der verschiedenen Metriken\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # 'weighted' berücksichtigt Label-Unausgewogenheit\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# ROC-AUC Score erfordert binarisierte Labels für Mehrklassenklassifikation\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_test)\n",
    "y_test_binarized = lb.transform(y_test)\n",
    "y_pred_binarized = lb.transform(y_pred)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test_binarized, y_pred_proba, multi_class='ovo', average='weighted')\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "# Ausgabe der Metriken\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}%\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Feature Importance extrahieren\n",
    "importance = xgb_clf.feature_importances_\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Feature Importance plotten\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importance)), importance)\n",
    "plt.xticks(range(len(importance)), feature_names, rotation=90)\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recursive Feature Elimination (RFE) is an effective method for feature selection that can enhance the performance of predictive models in statistics and machine learning. This technique aims to identify the most predictive features by successively removing the least important ones. The process starts with the complete set of features and iteratively eliminates the least significant feature, based on the model criterion chosen, until a predetermined number of features is reached or the model performance reaches an optimum.\n",
    "\n",
    "A key aspect of RFE is its ability to derive feature importance directly from the coefficients of models that provide intrinsic feature weightings, such as linear models and Support Vector Machines (Guyon et al., 2002). By incorporating Cross-Validation (CV) into the RFE process, known as RFECV, the robustness of feature selection is further enhanced by accounting for the variability of model performance across different data splits (Kohavi, 1995).\n",
    "\n",
    "The application of RFE can offer value across numerous domains, from bioinformatics to financial modeling, by not only improving model performance through the reduction of overfitting and enhancement of generalizability but also contributing to the interpretability of the models by highlighting the most relevant features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "# RFECV vorbereiten\n",
    "cv_strategy = StratifiedKFold(n_splits=5)\n",
    "rfecv = RFECV(estimator=xgb_clf, step=1, cv=cv_strategy, scoring='accuracy')\n",
    "\n",
    "# RFECV on train data\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "# Output the results of the feature selection\n",
    "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
    "\n",
    "# Extracting the feature names based on RFECV support\n",
    "selected_features = X_train.columns[rfecv.support_]\n",
    "\n",
    "# Output of the selected feature names and their rankings\n",
    "print(\"Selected features and their rankings:\")\n",
    "for rank, feature in zip(rfecv.ranking_, X_train.columns):\n",
    "    print(f\"{feature}: Rank {rank}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross Validation Score (Accuracy)\")\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])\n",
    "plt.title('RFECV - Performance of the model')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Die Confusion Matrix ist eine grundlegende und sehr informative Visualisierung,\n",
    "# die zeigt, wie gut das Modell die verschiedenen Klassen unterscheidet.\n",
    "# Sie gibt die Anzahl der korrekten und falschen Vorhersagen für jede Klasse an und hilft,\n",
    "# die Bereiche zu identifizieren, in denen das Modell möglicherweise verbessert werden muss\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Die Receiver Operating Characteristic (ROC)-Kurve und die Area Under the Curve (AUC)\n",
    "# sind besonders nützlich für binäre Klassifizierungsprobleme.\n",
    "# Sie können auch für Mehrklassenprobleme angepasst werden,\n",
    "# indem man die One-vs-All-Strategie für jede Klasse anwendet.\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Farben für die verschiedenen Klassen\n",
    "colors = cycle(['blue', 'red', 'green', 'orange', 'black'])\n",
    "\n",
    "# Plotten der ROC-Kurve für jede Klasse\n",
    "plt.figure(figsize=(7, 5))\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Berechnung für jede Klasse\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i],\n",
    "                                                        y_pred_proba[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test_binarized[:, i], y_pred_proba[:, i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " The Precision-Recall (PR) curve is an essential tool for assessing the effectiveness of classifiers, especially in situations with imbalanced datasets.\n",
    " Precision and Recall are two metrics used to evaluate the quality of a classification model. Precision indicates the proportion of instances classified as\n",
    " positive that are actually positive, while Recall indicates the proportion of actual positive instances that were classified as positive (Davis & Goadrich,\n",
    " 2006). \n",
    " The PR curve plots these two metrics in a coordinate system, with Recall on the X-axis and Precision on the Y-axis.\n",
    " The PR curve is particularly important in areas where the costs of false positives are very different or when the classes are imbalanced as in our project.\n",
    " In such cases, the PR curve can provide more detailed insights into the model's performance than other metrics, such as the ROC curve (Receiver Operating\n",
    " Characteristic Curve). \n",
    " The area under the PR curve (AUC-PR) can be considered a measure of the overall performance of the classification model (Saito & Rehmsmeier, 2015)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Die Precision-Recall-Curve\n",
    "plt.figure(figsize=(7, 5))\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "             label='Precision-Recall curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, average_precision[i]))\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Necessity of Applying GridSearchCV in Model Optimization\n",
    "\n",
    "The process of developing a predictive model involves not only selecting an appropriate algorithm but also fine-tuning its hyperparameters to optimize performance.\n",
    "GridSearchCV, a systematic approach for parameter tuning, plays a pivotal role in identifying the most effective combinations of hyperparameters for a given model.\n",
    "This method systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. \n",
    "The necessity of employing GridSearchCV can be substantiated by several key\n",
    "factors.\n",
    "\n",
    "Comprehensive Exploration\n",
    "GridSearchCV enables a comprehensive exploration of the parameter space, ensuring that the selected model is not just a product of arbitrary parameter choices but is instead optimized based on empirical evidence. \n",
    "This exhaustive search guarantees that the model's potential is fully realized by examining various permutations and combinations of parameters (Bergstra & Bengio, 2012).\n",
    "\n",
    "Enhanced Model Performance\n",
    "By meticulously searching through the parameter space, GridSearchCV often uncovers hyperparameter settings that significantly enhance model performance compared to the default\n",
    "settings. This optimization process is crucial for complex datasets where the optimal model configuration is not intuitively obvious (Claesen & De Moor, 2015).\n",
    "\n",
    "Bias-Variance Tradeoff Management\n",
    "The selection of hyperparameters has a profound impact on the bias-variance tradeoff. GridSearchCV assists in finding a balance, thereby reducing the risk of overfitting or\n",
    "underfitting. This balance is crucial for achieving generalizable model performance across unseen data (James et al., 2013).\n",
    "\n",
    "Reproducibility and Rigor\n",
    "Employing GridSearchCV adds rigor and reproducibility to the model development process. By documenting the explored parameter space and the performance of each combination\n",
    "researchers and practitioners provide a transparent account of the optimization process, facilitating replication and verification of the results (Varoquaux et al., 2015).\n",
    "\n",
    "Therefore, we use GridSearchCV with the grid parameters specified in the code to find out which parameters could be the best.\n",
    "However, it is also the case that parameters that do not appear in the array could be better.\n",
    "But we select these and no more because the calculation is quite time-consuming and resource-intensive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV-Initialisierung\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, scoring='accuracy', cv=4, n_jobs=-1)\n",
    "\n",
    "# Suche nach den besten Parametern\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "print(f\"Best Parameter: {grid_search.best_params_}\")\n",
    "print(f\"Best Accuracy: {grid_search.best_score_ * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "'learning_rate': 0.2: The learning rate determines how quickly or slowly a model learns from the errors in the training data. A learning rate of 0.2 is relatively high, meaning the model will converge faster, but there is a risk of it overshooting the optimum. For complex data sets, a higher learning rate can result in faster, but potentially less precise, adjustments.\n",
    "\n",
    "'max_depth': 5: The maximum depth of a tree limits how detailed the model can learn the data structures. A depth of 5 is a compromise between the model's ability to recognize patterns in the data and the risk of overfitting. It allows for some complexity in the model predictions without diving too deeply into the data.\n",
    "\n",
    "'n_estimators': 300: This number indicates how many trees to build in the ensemble model. 300 trees means the model tries to learn from the errors of 299 previous trees to improve predictions. A higher number of estimators can improve model performance, but also leads to longer training time and increased risk of overfitting if not controlled by other measures.\n",
    "\n",
    "'subsample': 1.0: This means that the model uses all available data when training each tree instead of taking a sample. This can improve performance if the training data is diverse and representative, but also runs the risk of overfitting since each tree sees exactly the same data.\n",
    "\n",
    " 'Accuracy = 52.73%': This number indicates the best accuracy achieved by the model using the hyperparameters mentioned above. An accuracy of just over 50% suggests that the model is only slightly\n",
    " better than random guessing, assuming a binary classification task. However, for multiclass classification as in our project or with a very balanced data set, this might represent acceptable\n",
    " performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_xgb_clf = XGBClassifier(**best_params, objective=\"multi:softprob\",use_label_encoder=False, eval_metric='mlogloss')\n",
    "best_xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_xgb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "roc_auc = roc_auc_score(y_test, best_xgb_clf.predict_proba(X_test), multi_class='ovo', average='weighted')\n",
    "\n",
    "print(f\"Accuracy on test data: {accuracy:.4f}%\")\n",
    "print(f\"F1-score on test data: {f1:.4f}%\")\n",
    "print(f\"ROC-AUC on test data: {roc_auc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: \n",
    "1. standard modell trainieren mit angepassten parametern (auswerten)\n",
    "2. FEATURES anpassen \n",
    "3. feature angepasstes modell nochmal trainieren ohne parameter (auswerten)\n",
    "4. feature angepasstes modell gridsearch cv \n",
    "5. feature angepasstes modell mit angepassten parametern trainieren (auswerten)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Idee, das Feature mit dem höchsten Score entfernen wir, um zu überpüfen, ob sich Änderungen im Modell feststellen\n",
    "\n",
    "X_train = X_train.drop(columns=['PdDistrict_TENDERLOIN'])\n",
    "X_test = X_test.drop(columns=['PdDistrict_TENDERLOIN'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(objective=\"multi:softprob\",use_label_encoder=False, eval_metric='mlogloss')\n",
    "results = perform_cross_validation(xgb_clf, X_train, y_train)\n",
    "\n",
    "for key, value in results.items():\n",
    " print(f\"{key}: {value:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " xgb_clf, accuracy, f1, roc_auc = evaluate_xgb_model(X_train, y_train, X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}%\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "importance = xgb_clf.feature_importances_\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Feature Importance plotten\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importance)), importance)\n",
    "plt.xticks(range(len(importance)), feature_names, rotation=90)\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_params, best_accuracy = perform_grid_search(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_clf, accuracy, f1, roc_auc = evaluate_xgb_model(X_train, y_train, X_test, y_test, best_params)\n",
    "print(f\"Accuracy on test data: {accuracy:.4f}%\")\n",
    "print(f\"F1-score on test data: {f1:.4f}%\")\n",
    "print(f\"ROC-AUC on test data: {roc_auc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**References:**\n",
    "\n",
    "- T. Chen & C. Guestrin (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\n",
    "- C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer.\n",
    "- T. Hastie, R. Tibshirani, & J. Friedman (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics.\n",
    "- G. James, D. Witten, T. Hastie, & R. Tibshirani (2013). An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics.\n",
    "- J.H. Friedman (2001). Greedy Function Approximation: A Gradient Boosting Machine. The Annals of Statistics.\n",
    "- Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. Machine Learning, 46(1-3), 389-422.\n",
    "- Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. IJCAI'95: Proceedings of the 14th international joint conference on Artificial intelligence, 2(12), 1137-1145.\n",
    "- Databasecamp. (2023) https://databasecamp.de/statistik/f1-score\n",
    "- Davis, J., & Goadrich, M. (2006). The relationship between Precision-Recall and ROC curves. In Proceedings of the 23rd international conference on Machine learning (pp. 233-240).\n",
    "- Saito, T., & Rehmsmeier, M. (2015). The Precision-Recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets.\n",
    "- (2023) https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432\n",
    "- Kohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. International Joint Conference on Artificial Intelligence.\n",
    "- Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene Selection for Cancer Classification using Support Vector Machines. Machine Learning, 46(1-3), 389-422.\n",
    "- Powers, D.M.W. (2011). Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness & Correlation. Journal of Machine Learning Technologies, 2(1), 37-63.\n",
    "- Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb), 281-305.\n",
    "- Claesen, M., & De Moor, B. (2015). Hyperparameter search in machine learning. arXiv preprint arXiv:1502.02127 Titel anhand dieser ArXiv-ID in Citavi-Projekt übernehmen.\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.\n",
    "- Varoquaux, G., Buitinck, L., Louppe, G., Grisel, O., Pedregosa, F., & Mueller, A. (2015). Scikit-learn: Machine learning without learning the machinery. GetMobile: Mobile Computing and Communications, 19(1), 29-33."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
