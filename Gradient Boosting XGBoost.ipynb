{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, log_loss, mean_squared_error\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from TrainTestData import loadData\n",
    "\n",
    "from Features import FeatureColumns\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T17:33:21.023759500Z",
     "start_time": "2024-02-17T17:33:20.967687700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, data, feature_columns, categorical_features,target_column = loadData()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T17:33:23.944663800Z",
     "start_time": "2024-02-17T17:33:20.983016400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the preprocessing stage of our multi-class classification pipeline, we employ a crucial step of transforming categorical target variables into a numerical format. This transformation is essential for compatibility with XGBoost, a gradient boosting framework that requires numerical input to perform mathematical operations and optimizations integral to its learning algorithm (Chen & Guestrin, 2016).\n",
    "XGBoost, like many machine learning algorithms, operates on numerical data to execute arithmetic operations, gradient calculations, and optimization procedures. These operations are not inherently defined for categorical data, necessitating a transformation of categorical labels into a numerical representation (Bishop, 2006).\n",
    "Specifically, for a multi-class classification problem, each unique categorical class label is assigned a unique integer value, a process known as label encoding. This procedure transforms the categorical target variable into a format amenable to the mathematical computations required by XGBoost (Hastie, Tibshirani, & Friedman, 2009).\n",
    "For a dataset with \\(C\\) unique classes, label encoding maps each class to a unique integer in the range \\([0, C-1]\\). This approach is both efficient and straightforward, ensuring that the transformed target variable retains the essential categorical information in a numerical format compatible with XGBoost's requirements (James, Witten, Hastie, & Tibshirani, 2013).\n",
    "Upon transformation, XGBoost utilizes the numerical labels to compute a multi-class log loss (cross-entropy loss) when the `multi:softprob` or `multi:softmax` objective function is specified. This loss function quantifies the difference between the predicted probabilities and the actual class labels, guiding the model's learning process (Friedman, 2001).\n",
    "The transformation of categorical data into numerical form is supported by the foundational principles of statistical learning, which emphasize the necessity of numerical representation for the application of mathematical models to data analysis and prediction tasks (Bishop, 2006; Hastie, Tibshirani, & Friedman, 2009). Furthermore, research by Chen and Guestrin (2016) on XGBoost highlights the effectiveness of gradient boosting machines for various tasks, including multi-class classification, when data is appropriately preprocessed and encoded.\n",
    "Data transformation step is not merely a technical requirement but a scientifically grounded practice that enhances the ability of XGBoost to accurately model and predict outcomes in multi-class classification settings. By converting categorical labels into a numerical format, we align our dataset with the underlying mathematical framework of machine learning algorithms, facilitating efficient and effective model training and prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#initialise coder and classifier\n",
    "xgb_clf = XGBClassifier(objective=\"multi:softprob\",use_label_encoder=False, eval_metric='mlogloss')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T17:33:23.958735400Z",
     "start_time": "2024-02-17T17:33:23.947169Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5035\n",
      "F1-Score: 0.4466\n",
      "Precision: 0.4481\n",
      "Recall: 0.5035\n",
      "ROC-AUC Score: 0.6889\n",
      "Log Loss: 1.3068\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training des Classifiers\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen auf dem Testset\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Vorhersagen der Wahrscheinlichkeiten für das Testset, um ROC-AUC und Log Loss zu berechnen\n",
    "y_pred_proba = xgb_clf.predict_proba(X_test)\n",
    "\n",
    "# Berechnung der verschiedenen Metriken\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # 'weighted' berücksichtigt Label-Unausgewogenheit\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# ROC-AUC Score erfordert binarisierte Labels für Mehrklassenklassifikation\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_test)\n",
    "y_test_binarized = lb.transform(y_test)\n",
    "y_pred_binarized = lb.transform(y_pred)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test_binarized, y_pred_proba, multi_class='ovo', average='weighted')\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "# Ausgabe der Metriken\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Log Loss: {logloss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T17:33:26.862649600Z",
     "start_time": "2024-02-17T17:33:23.963255200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#initialise coder and classifier\n",
    "xgb_clf = XGBClassifier(objective=\"multi:softprob\",use_label_encoder=False, eval_metric='mlogloss')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T17:33:26.882117Z",
     "start_time": "2024-02-17T17:33:26.863649300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recursive Feature Elimination (RFE) is an effective method for feature selection that can enhance the performance of predictive models in statistics and machine learning. This technique aims to identify the most predictive features by successively removing the least important ones. The process starts with the complete set of features and iteratively eliminates the least significant feature, based on the model criterion chosen, until a predetermined number of features is reached or the model performance reaches an optimum.\n",
    "\n",
    "A key aspect of RFE is its ability to derive feature importance directly from the coefficients of models that provide intrinsic feature weightings, such as linear models and Support Vector Machines (Guyon et al., 2002). By incorporating Cross-Validation (CV) into the RFE process, known as RFECV, the robustness of feature selection is further enhanced by accounting for the variability of model performance across different data splits (Kohavi, 1995).\n",
    "\n",
    "The application of RFE can offer value across numerous domains, from bioinformatics to financial modeling, by not only improving model performance through the reduction of overfitting and enhancement of generalizability but also contributing to the interpretability of the models by highlighting the most relevant features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "# RFECV vorbereiten\n",
    "cv_strategy = StratifiedKFold(n_splits=4)\n",
    "rfecv = RFECV(estimator=xgb_clf, step=1, cv=cv_strategy, scoring='accuracy')\n",
    "\n",
    "# RFECV on train data\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "# Output the results of the feature selection\n",
    "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
    "\n",
    "# Extracting the feature names based on RFECV support\n",
    "selected_features = X_train.columns[rfecv.support_]\n",
    "\n",
    "# Output of the selected feature names and their rankings\n",
    "print(\"Selected features and their rankings:\")\n",
    "for rank, feature in zip(rfecv.ranking_, X_train.columns):\n",
    "    print(f\"{feature}: Rank {rank}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross Validation Score (Accuracy)\")\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])\n",
    "plt.title('RFECV - Performance of the model')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-17T17:33:26.891309700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Scoring-Kriterien definieren\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'roc_auc_ovr': 'roc_auc_ovr'\n",
    "}\n",
    "\n",
    "# Kreuzvalidierung mit mehreren Metriken\n",
    "cv_results = cross_validate(xgb_clf, X_train, y_train, cv=4, scoring=scoring, return_train_score=False)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "for metric in scoring.keys():\n",
    "    print(f\"Average {metric}: {np.mean(cv_results[f'test_{metric}']) * 100:.2f}%\")\n",
    "    print(f\"Standard deviation {metric}: {np.std(cv_results[f'test_{metric}']) * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Die Confusion Matrix ist eine grundlegende und sehr informative Visualisierung,\n",
    "# die zeigt, wie gut das Modell die verschiedenen Klassen unterscheidet.\n",
    "# Sie gibt die Anzahl der korrekten und falschen Vorhersagen für jede Klasse an und hilft,\n",
    "# die Bereiche zu identifizieren, in denen das Modell möglicherweise verbessert werden muss\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Die Receiver Operating Characteristic (ROC)-Kurve und die Area Under the Curve (AUC)\n",
    "# sind besonders nützlich für binäre Klassifizierungsprobleme.\n",
    "# Sie können auch für Mehrklassenprobleme angepasst werden,\n",
    "# indem man die One-vs-All-Strategie für jede Klasse anwendet.\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Farben für die verschiedenen Klassen\n",
    "colors = cycle(['blue', 'red', 'green', 'orange', 'black'])\n",
    "\n",
    "# Plotten der ROC-Kurve für jede Klasse\n",
    "plt.figure(figsize=(7, 5))\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Berechnung für jede Klasse\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i],\n",
    "                                                        y_pred_proba[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "\n",
    "# Die Precision-Recall-Kurve ist eine weitere nützliche Visualisierung, \n",
    "# besonders in Situationen, in denen die Klassen stark unausgeglichen sind. \n",
    "# Sie zeigt das Verhältnis von Precision (Präzision) und Recall (Sensitivität) für verschiedene Schwellenwerte.\n",
    "plt.figure(figsize=(7, 5))\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "             label='Precision-Recall curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, average_precision[i]))\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**References:**\n",
    "\n",
    "- T. Chen & C. Guestrin (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\n",
    "- C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer.\n",
    "- T. Hastie, R. Tibshirani, & J. Friedman (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics.\n",
    "- G. James, D. Witten, T. Hastie, & R. Tibshirani (2013). An Introduction to Statistical Learning: with Applications in R. Springer Texts in Statistics.\n",
    "- J.H. Friedman (2001). Greedy Function Approximation: A Gradient Boosting Machine. The Annals of Statistics.\n",
    "- Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. Machine Learning, 46(1-3), 389-422.\n",
    "- Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. IJCAI'95: Proceedings of the 14th international joint conference on Artificial intelligence, 2(12), 1137-1145.\n",
    "- Databasecamp. (2023) https://databasecamp.de/statistik/f1-score"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
